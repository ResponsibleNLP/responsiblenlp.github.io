<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | NLP4Health Lab Amsterdam </title> <meta name="author" content="Iacer Calixto"> <meta name="description" content="The NLP4Health Lab Amsterdam is located at the Amsterdam UMC in the University of Amsterdam and conducts cutting-edge research on human-centric and responsible Natural Language Processing (NLP) and Machine Learning (ML) methods for high-stakes applications with a strong focus on healthcare applications. The Lab is lead by dr. Iacer Calixto. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nlp4health-lab.github.io/team/principal-investigator"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> NLP4Health Lab Amsterdam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">people </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/team/">our team</a> <a class="dropdown-item " href="/team/principal-investigator">principal investigator</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"></h1> <p class="post-description"></p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/team_icalixto_600x450px-480.webp 480w,/assets/img/team_icalixto_600x450px-800.webp 800w,/assets/img/team_icalixto_600x450px-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/team_icalixto_600x450px.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Iacer Calixto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"></div> </div> <div class="caption"> </div> <p><strong>Prospective students:</strong> please read <a href="/team/principal-investigator/note-prospective-students">this note</a> before emailing me.</p> <h4 id="background">Background.</h4> <p>I am assistant professor at the Department of Medical Informatics, Amsterdam UMC location AMC, University of Amsterdam (UvA). I was recently awarded an <a href="https://www.amsterdamumc.org/en/spotlight/building-models-to-enable-greater-use-of-ai-in-the-health-care-system.htm" rel="external nofollow noopener" target="_blank">NWO AiNed Fellowship</a> and lead the NLP4Health Lab Amsterdam. Before that, I was a <a href="https://cordis.europa.eu/project/id/838188" rel="external nofollow noopener" target="_blank">Marie-Curie Global Fellow</a> in the <a href="https://www.illc.uva.nl/" rel="external nofollow noopener" target="_blank">Institute for Logic, Language and Computation (ILLC) at the UvA</a> and <a href="https://wp.nyu.edu/ml2/" rel="external nofollow noopener" target="_blank">New York University’s Center for Data Science</a>. I obtained my PhD from Dublin City University where I was a member of the <a href="https://www.adaptcentre.ie/" rel="external nofollow noopener" target="_blank">ADAPT Centre</a>. I also hold an Erasmus Mundus Master in Natural Language Processing and Human Language Technology, a MSc degree in Computer Science, and a BSc degree in Information Systems.</p> <h4 id="research">Research.</h4> <p>I work on human-centric and responsible Natural Language Processing (NLP) and Machine Learning (ML) methods for high-stakes applications with a strong focus on healthcare. My team and I propose methods with a focus on the <em>Dutch and European healthcare ecosystems</em>. Some of the research topics I am interested in include NLP and ML methods that are <em>transparent, interpretable, and explainable</em>, that can <em>efficiently unlearn information</em>, that <em>ensure fairness and (patient) privacy</em>, that <em>prevent and mitigate bias</em>, that <em>cope with data scarcity</em> and that <em>generalise across (patient) distributions and (downstream) tasks</em>.</p> <p>My main research line tackles challenges surrounding clinical NLP, but I also research methods to model the complex interaction between language and other modalities (e.g., images, videos, audio), knowledge graphs, world, and commonsense knowledge.</p> <h4 id="other">Other.</h4> <p>I am/have been/will be area chair for ARR 2024 (February), senior PC member for ECAI 2024 and AAAI 2024, co-organiser of the SemEval 2023 Visual Word Sense Disambiguation shared task, area chair for EACL 2021, and co-organiser of the Representation Learning for NLP (RepL4NLP) workshop 2021 (co-located with ACL 2021). I am a faculty member of the European Laboratory for Learning and Intelligent Systems (ELLIS) and a member of the Association for Computational Linguistics (ACL).</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ViLMA-Figure1-480.webp 480w,/assets/img/publication_preview/ViLMA-Figure1-800.webp 800w,/assets/img/publication_preview/ViLMA-Figure1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/ViLMA-Figure1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ViLMA-Figure1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="kesen-etal-2024vilma" class="col-sm-8"> <div class="title">ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models</div> <div class="author"> Ilker Kesen ,  Andrea Pedrotti ,  Mustafa Dogan ,  Michele Cafagna ,  Emre Can Acikgoz ,  Letitia Parcalabescu ,  <em>Iacer Calixto</em> ,  Anette Frank , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Albert Gatt, Aykut Erdem, Erdem Erkut' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em> , May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=liuqDwmbQJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kesen-etal-2024vilma</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vi{LMA}: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kesen, Ilker and Pedrotti, Andrea and Dogan, Mustafa and Cafagna, Michele and Acikgoz, Emre Can and Parcalabescu, Letitia and Calixto, Iacer and Frank, Anette and Gatt, Albert and Erdem, Aykut and Erkut, Erdem}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=liuqDwmbQJ}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/VidLM_Table1-480.webp 480w,/assets/img/publication_preview/VidLM_Table1-800.webp 800w,/assets/img/publication_preview/VidLM_Table1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/VidLM_Table1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="VidLM_Table1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Zonneveld_2023_ICCV" class="col-sm-8"> <div class="title">Video-and-Language (VidL) models and their cognitive relevance</div> <div class="author"> Anne Zonneveld ,  Albert Gatt ,  and  <em>Iacer Calixto</em> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em> , Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://sites.google.com/view/iccv-mmfm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In this paper we give a narrative review of multi-modal video-language (VidL) models. We introduce the current landscape of VidL models and benchmarks, and draw inspiration from neuroscience and cognitive science to propose avenues for future research in VidL models in particular and artificial intelligence (AI) in general. We argue that iterative feedback loops between AI, neuroscience, and cognitive science are essential to spur progress across these disciplines. We motivate why we focus specifically on VidL models and their benchmarks as a promising type of model to bring improvements in AI and categorise current VidL efforts across multiple’cognitive relevance axioms’. Finally, we provide suggestions on how to effectively incorporate this interdisciplinary viewpoint into research on VidL models in particular and AI in general. In doing so, we hope to create awareness of the potential of VidL models to narrow the gap between neuroscience, cognitive science, and AI.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Zonneveld_2023_ICCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zonneveld, Anne and Gatt, Albert and Calixto, Iacer}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video-and-Language (VidL) models and their cognitive relevance}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{325-338}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Soft-prompt-tuning_Fig1-480.webp 480w,/assets/img/publication_preview/Soft-prompt-tuning_Fig1-800.webp 800w,/assets/img/publication_preview/Soft-prompt-tuning_Fig1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/Soft-prompt-tuning_Fig1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Soft-prompt-tuning_Fig1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1007/978-3-031-34344-5_23" class="col-sm-8"> <div class="title">Soft-Prompt Tuning to Predict Lung Cancer Using Primary Care Free-Text Dutch Medical Notes</div> <div class="author"> Auke Elfrink ,  Iacopo Vagliano ,  Ameen Abu-Hanna ,  and  <em>Iacer Calixto</em> </div> <div class="periodical"> <em>In Artificial Intelligence in Medicine</em> , Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-34344-5_23" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/src/master/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We examine the use of large Transformer-based pretrained language models (PLMs) for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Specifically, we investigate: 1) how soft prompt-tuning compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. All our code is available open source in https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1007/978-3-031-34344-5_23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Elfrink, Auke and Vagliano, Iacopo and Abu-Hanna, Ameen and Calixto, Iacer}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Juarez, Jose M. and Marcos, Mar and Stiglic, Gregor and Tucker, Allan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Soft-Prompt Tuning to Predict Lung Cancer Using Primary Care Free-Text Dutch Medical Notes}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Artificial Intelligence in Medicine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{193--198}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-34344-5}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Visual-WSD_Figure1-480.webp 480w,/assets/img/publication_preview/Visual-WSD_Figure1-800.webp 800w,/assets/img/publication_preview/Visual-WSD_Figure1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/Visual-WSD_Figure1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Visual-WSD_Figure1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="raganato-etal-2023-semeval" class="col-sm-8"> <div class="title">SemEval-2023 Task 1: Visual Word Sense Disambiguation</div> <div class="author"> Alessandro Raganato ,  <em>Iacer Calixto</em> ,  Asahi Ushio ,  Jose Camacho-Collados ,  and  Mohammad Taher Pilehvar </div> <div class="periodical"> <em>In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)</em> , Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.semeval-1.308/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>This paper presents the Visual Word Sense Disambiguation (Visual-WSD) task. The objective of Visual-WSD is to identify among a set of ten images the one that corresponds to the intended meaning of a given ambiguous word which is accompanied with minimal context. The task provides datasets for three different languages: English, Italian, and Farsi.We received a total of 96 different submissions. Out of these, 40 systems outperformed a strong zero-shot CLIP-based baseline. Participating systems proposed different zero- and few-shot approaches, often involving generative models and data augmentation. More information can be found on the task’s website: }urlhttps://raganato.github.io/vwsd/.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">raganato-etal-2023-semeval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{S}em{E}val-2023 Task 1: Visual Word Sense Disambiguation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Raganato, Alessandro and Calixto, Iacer and Ushio, Asahi and Camacho-Collados, Jose and Pilehvar, Mohammad Taher}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.semeval-1.308}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.semeval-1.308}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2227--2234}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NNLG_Figure1-480.webp 480w,/assets/img/publication_preview/NNLG_Figure1-800.webp 800w,/assets/img/publication_preview/NNLG_Figure1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/NNLG_Figure1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="NNLG_Figure1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1613/jair.1.12918" class="col-sm-8"> <div class="title">Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning</div> <div class="author"> Erkut Erdem ,  Menekse Kuyu ,  Semih Yagcioglu ,  Anette Frank ,  Letitia Parcalabescu ,  Barbara Plank ,  Andrii Babii ,  Oleksii Turuta , and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Aykut Erdem, Iacer Calixto, Elena Lloret, Elena-Simona Apostol, Ciprian-Octavian Truică, Branislava Šandrih, Sanda Martinčić-Ipšić, Gábor Berend, Albert Gatt, Grăzina Korvel' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>J. Artif. Int. Res.</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.jair.org/index.php/jair/article/view/12918" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1613/jair.1.12918" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1613/jair.1.12918</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Erdem, Erkut and Kuyu, Menekse and Yagcioglu, Semih and Frank, Anette and Parcalabescu, Letitia and Plank, Barbara and Babii, Andrii and Turuta, Oleksii and Erdem, Aykut and Calixto, Iacer and Lloret, Elena and Apostol, Elena-Simona and Truic\u{a}, Ciprian-Octavian and \v{S}andrih, Branislava and Martin\v{c}i\'{c}-Ip\v{s}i\'{c}, Sanda and Berend, G\'{a}bor and Gatt, Albert and Korvel, Gr\u{a}zina}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{May 2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{AI Access Foundation}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{El Segundo, CA, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{73}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1076-9757}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1613/jair.1.12918}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.12918}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{J. Artif. Int. Res.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{77}</span><span class="p">,</span>
  <span class="na">alt_metric</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{natural language, neural networks}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/VALSE_Table1-480.webp 480w,/assets/img/publication_preview/VALSE_Table1-800.webp 800w,/assets/img/publication_preview/VALSE_Table1-1400.webp 1400w," sizes="80vw,80vh" type="image/webp"> <img src="/assets/img/publication_preview/VALSE_Table1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="VALSE_Table1.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="parcalabescu-etal-2022-valse" class="col-sm-8"> <div class="title">VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena</div> <div class="author"> Letitia Parcalabescu ,  Michele Cafagna ,  Lilitta Muradjan ,  Anette Frank ,  <em>Iacer Calixto</em> ,  and  Albert Gatt </div> <div class="periodical"> <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.acl-long.567/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/heidelberg-nlp/valse" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&amp;L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&amp;L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&amp;L models from a linguistic perspective, complementing the canonical task-centred V&amp;L evaluations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">parcalabescu-etal-2022-valse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{VALSE}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Dublin, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.acl-long.567}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.acl-long.567}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8253--8280}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="sticky-bottom mt-5 social" role="contentinfo"> <div class="container"> © Copyright 2024 Iacer Calixto. <a href="https://nlp4health-lab.github.io/">Impressum</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>